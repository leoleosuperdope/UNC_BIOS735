---
title: "Homework 5 - Optimization"
author: "Leo Li (PID: 730031954)"
date: "`r format(Sys.time(), '%m/%d/%Y')`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Question 1:  Simple Univariate Optimization

Use Newton-Raphson to maximize the following function:  $$f(x) = 7\log(x) + 3\log(1-x).$$  Fill in the R functions below pertaining to $f(x)$, $f'(x)$ and $f''(x)$.  Utilize the values below to initialize and run the algorithm (analogous to how they were defined in class).  Repeat the method for several starting values for $x$ given below, picking the best $\hat{x}$ in terms of the final objective function value for each. Make sure you print your final value for $\hat{x}$

```{r}
# f(x)
f = function(x){
  value = 7*log(x) + 3*log(1-x)
  return(value)
}
# first derivative
f1 = function(x){
  first = 7*x^(-1)-3*(1-x)^(-1)
  return(first)
}
# second derivative
f2 = function(x){
  second = -7*x^(-2) - 3*(1-x)^(-2)
  return(second)
}

# initial value: x = 0.01
tol = 10^-4
x = 0.01 
maxit = 50
iter = 0
eps = Inf
while (eps > tol & iter < maxit) {
  x0 = x
  h = -f1(x) / f2(x)
  x = x + h
  fval = f(x)
  eps  = abs(x - x0)
  iter = iter + 1
  if (iter == maxit)
    warning("Iteration limit reached without convergence")
  cat(sprintf("Iter: %d function: %f x: %f h: %f eps:%f\n", iter, fval, x, h, eps))
}

# initial value: x = 0.5
tol = 10^-4
x = 0.5 
maxit = 50
iter = 0
eps = Inf
while (eps > tol & iter < maxit) {
  x0 = x
  h = -f1(x) / f2(x)
  x = x + h
  fval = f(x)
  eps  = abs(x - x0)
  iter = iter + 1
  if (iter == maxit)
    warning("Iteration limit reached without convergence")
  cat(sprintf("Iter: %d function: %f x: %f h: %f eps:%f\n", iter, fval, x, h, eps))
}

# initial value: x = 0.99
tol = 10^-4
x = 0.99 
maxit = 50
iter = 0
eps = Inf
while (eps > tol & iter < maxit) {
  x0 = x
  h = -f1(x) / f2(x)
  x = x + h
  fval = f(x)
  eps  = abs(x - x0)
  iter = iter + 1
  if (iter == maxit)
    warning("Iteration limit reached without convergence")
  cat(sprintf("Iter: %d function: %f x: %f h: %f eps:%f\n", iter, fval, x, h, eps))
}
```

Based on the output, no matter whether we choose 0.01, 0.5, or 0.99 as our initial value, the value of $\hat{x}$ will always be obtained as 0.7. 

Bonus:  $f(x)$ pertains to the likelihood/PDF for which distribution(s)?  Two answers are possible.  Given this, what would be a closed form estimate for $\hat{x}$?

$f(x)$ pertains to the likelihood of binomial distribution when $n=10$ and $y=7$, where $n$ represents the number of trials and $y$ represents the number of successes. In this case, a closed form estimate for $\hat{x}$ is that $\hat{x} = \frac{7}{10} = 0.7$, which agrees to our solution resulted from Newton-Raphson. 

# Question 2:  Not So Simple Univariate Optimization

Repeat question 1 for the function below, using the same template for your answer.  Choose a range of starting values between 0 and 5 and report the best $\hat{x}$ based on the final objective function value:

$$f(x) = 1.95 - e^{-2/x} - 2e^{-x^4}.$$

```{r}
# f(x)
f = function(x){
  value = 1.95 - exp(-2/x) - 2*exp(-x^4)
  return(value)
}
# first derivative
f1 = function(x){
  first = -2*x^(-2)*exp(-2/x) + 8*x^3*exp(-x^4)
  return(first)
}
# second derivative
f2 = function(x){
  second = 8*(-4*exp(-x^4)*x^6 + 3*exp(-x^4)*x^2) - 2*(2*exp(-2/x)-2*exp(-2/x)*x)/(x^4)
  return(second)
}

# initial value: x = 0.5
tol = 10^-4
x = 0.5
maxit = 50
iter = 0
eps = Inf
while (eps > tol & iter < maxit) {
  x0 = x
  h = -f1(x) / f2(x)
  x = x + h
  fval = f(x)
  eps  = abs(x - x0)
  iter = iter + 1
  if (iter == maxit)
    warning("Iteration limit reached without convergence")
  cat(sprintf("Iter: %d function: %f x: %f h: %f eps:%f\n", iter, fval, x, h, eps))
}

# initial value: x = 1.5
tol = 10^-4
x = 1.5
maxit = 50
iter = 0
eps = Inf
while (eps > tol & iter < maxit) {
  x0 = x
  h = -f1(x) / f2(x)
  x = x + h
  fval = f(x)
  eps  = abs(x - x0)
  iter = iter + 1
  if (iter == maxit)
    warning("Iteration limit reached without convergence")
  cat(sprintf("Iter: %d function: %f x: %f h: %f eps:%f\n", iter, fval, x, h, eps))
}

# initial value: x = 2.5
tol = 10^-4
x = 2.5
maxit = 50
iter = 0
eps = Inf
while (eps > tol & iter < maxit) {
  x0 = x
  h = -f1(x) / f2(x)
  x = x + h
  fval = f(x)
  eps  = abs(x - x0)
  iter = iter + 1
  if (iter == maxit)
    warning("Iteration limit reached without convergence")
  cat(sprintf("Iter: %d function: %f x: %f h: %f eps:%f\n", iter, fval, x, h, eps))
}
```

Based on the final objective function value, the best $\hat{x}$ equals to 1.47.

What does this say about the stability of NR, especially in this case?  Plot the second derivative of this function and comment on why or why this is supports your observations.

```{r}
plot(c(1:500)*0.01, f2(c(1:500)*0.01), xlab="x", ylab="f''(x)")
```

From this case, we can see that the optimization using NR is relatively unstable. In particular, it is very sensitive to the choice of starting values especially in the scenario in which the objective function is not convex. The plot of the second derivative supports the observation, because in the interval [0, 5], there exist some points at which the second derivative equals zero.

## Multivariate optimization:  Zero-inflated Poisson 

Following a chemical spill from a local industrial plant, the city government is attempting to characterize the impact of the spill on recreational fishing at a nearby lake.  Over the course of one month following the spill, park rangers asked each adult leaving the park how many fish they had caught that day. At the end of the month they had collected responses for 4,075 individuals, where the number of fish caught for each individual is summarized in the table below:

```{r, echo=F}
library(knitr)
kable(matrix(c(3062, 587, 284, 103, 33, 4, 2), nrow = 1, byrow = T),col.names = as.character(0:6),format = "html", table.attr = "style='width:30%;'",)
```

Based on an initial examination of this distribution, there appears to be many more zeros than expected.  Upon speaking to the park rangers, they were embarrassed to state that after recording the number of fish each adult had caught, they did not ask or record whether each adult had gone to the park with the intention of fishing in the first place.  

The city statistician surmised that the observed distribution in the table above may be resulting from a mixture of two populations of subjects.  The first population pertains to the subset of visitors that arrived without any intention of fishing (exactly zero fish caught by each member of this population). The second population pertains to the set of visitors that arrived with the intention of fishing (0 or more fish potentially caught in this population).  Therefore, if we fit a standard Poisson model to this data, the estimate for $\lambda$ would be biased downwards.  

To account for the excess zeros in the observed data, the statistician decided to fit a zero-inflated Poisson model.  To simplify things the log likelihood is given below, and we utilized the tabulated values from the table above in place of individual observations: 

$$ 
\mathcal{l}(\boldsymbol{\theta}) = n_0\log(\pi + (1-\pi)e^{-\lambda}) + (N-n_0)[\log(1-\pi) - \lambda] + \sum_{i=1}^{\infty}in_i\log(\lambda)
$$

where $\boldsymbol{\theta} = (\pi, \lambda)$, $n_i$ pertains to the number of individuals that caught $i$ fish ($n_i=0$ for $i>6$ here), $N = 4075$, $\pi$ is the probability that an individual did not show up to fish, and $\lambda$ is the mean number of fish caught by those individuals that intended to fish at the park.  From this log-likelihood, we can see that for the individuals that caught 0 fish, we assume those observations come from a mixture of individuals that did not show up to fish ($\pi$ proportion of the time) and those that showed up to fish but did not catch anything ($1-\pi$ proportion of the time with 0 for their catch count).  The remaining individuals that caught more than zero fish are also in the log likelihood and are similarly weighted by $(1-\pi)$. 


Lets go ahead and fit this model with Newton-Raphson.  Similar to the previous problems, fill in the code below to fit this model.  Then, fill in the function for the log likelihood and its 1st derivative $$\left(\frac{\partial\mathcal{l}(\boldsymbol{\theta})}{\partial\pi}, \frac{\partial\mathcal{l}(\boldsymbol{\theta})}{\partial\lambda}\right)$$ and second derivative matrix 

$$\left[\begin{array}
{rr}
\frac{\partial^2\mathcal{l}(\boldsymbol{\theta})}{\partial\pi^2} & \frac{\partial^2\mathcal{l}(\boldsymbol{\theta})}{\partial\pi \partial\lambda} \\
\frac{\partial^2\mathcal{l}(\boldsymbol{\theta})}{\partial\lambda \partial\pi } & \frac{\partial^2\mathcal{l}(\boldsymbol{\theta})}{\partial\lambda^2} 
\end{array}\right] $$  

Given the estimates, interpret the results. Accounting for the excess zeros, on average how many fish were caught among those who intended to go fishing?  What proportion of subjects did not even consider to fish?


```{r}
# f(x)
logLik = function(theta, y, ny){
  value = ny[1]*log(theta[1]+(1-theta[1])*exp(-theta[2])) 
  +(sum(ny)-ny[1])*(log(1-theta[1]) - theta[2]) + sum(ny*y)*log(theta[2])
  return(value)
  }
# first derivative
f1 = function(theta,y, ny){
  N = sum(ny)
  n0 = ny[1]
  pi = theta[1]
  lambda = theta[2]
  big.sum = sum(ny*y)
  first1 = (-N*pi + N*exp(-lambda)*pi +n0 
            -N*exp(-lambda))/((-pi+1)*(pi+exp(-lambda)*(-pi+1)))
  first2 = -(n0*exp(-lambda)*(-pi
            +1))/(pi+exp(-lambda)*(1-pi)) - N + n0 + big.sum/lambda
  return(as.matrix(c(first1, first2), nrow=2))
  }
# second derivative
f2 = function(theta,y, ny){
  N = sum(ny)
  n0 = ny[1]
  pi = theta[1]
  lambda = theta[2]
  big.sum = sum(ny*y)
  second11 = -(n0*(-exp(-lambda)+1)^2)/((pi+exp(-lambda)*(1-pi))^2) - (N-n0)/((1-pi)^2)
  second12 = (n0*exp(-lambda))/((pi + exp(-lambda)*(-pi + 1))^2)
  second22 = (n0*exp(-lambda)*pi*(-pi+1))/((pi+exp(-lambda)*(-pi+1))^2) - big.sum/(lambda^2)
  return(matrix(c(second11, second12, second12, second22), nrow=2))
}

# data 
y = 0:6
ny = c(3062, 587, 284, 103, 33, 4, 2)

# to start the model
tol = 10^-4
theta = c(
          ny[y==0]/sum(ny), # initial value for pi, prop of total 0's
          sum(ny*y)/sum(ny) # intial value for lambda, mean of y's
          )
maxit = 50
iter = 0
eps = Inf

start = Sys.time()
while (eps > tol & iter < maxit) {
  theta0 = theta
  h = -solve(f2(theta, y, ny)) %*% f1(theta, y, ny)
  theta = theta + h
  logL = logLik(theta, y, ny)
  eps  = sqrt(sum((theta - theta0) ^ 2))
  iter = iter + 1
  if (iter == maxit)
    warning("Iteration limit reached without convergence")
  cat(
    sprintf(
      "Iter: %d logL: %.2f pi: %.3f lambda: %.3f eps:%f\n",
      iter,
      logL,
      theta[1],
      theta[2],
      eps
    )
  )
}
```

Given the estimate, we can interpret the results as follows. 0.615 of the subjects did not consider to fish, and for the remaining subjects who intended to go fishing, the number of fish caught follows a Poisson distribution with mean of 1.038. Therefore, accounting for the excess zeros, on average 1.038 fish were caught among those who intended to go fishing. 





