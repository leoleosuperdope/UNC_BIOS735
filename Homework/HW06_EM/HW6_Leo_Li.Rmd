---
title: "Homework 6 - EM"
author: "Leo Li (PID: 730031954)"
date: "`r format(Sys.time(), '%m/%d/%Y')`"
output: html_document
header_includes:
- \usepackage{amsmath}
- \usepackage{amssymb}
- \usepackage{amsthm}
include-before:
- '\newcommand{\bfm}[1]{\ensuremath{\mathbf{#1}}}'
- '\newcommand{\bdm}[1]{\ensuremath{\boldsymbol{#1}}}'
- '$\def \d \bfm{d}$'
- '$\def \e \bfm{e}$'
- '$\def \g \bfm{g}$'
- '$\def \I \bfm{I}$'
- '$\def \l \bfm{l}$'
- '$\def \M \bfm{M}$'
- '$\def \W \bfm{W}$'
- '$\def \y \bfm{y}$'
- '$\def \Y \bfm{Y}$'
- '$\def \x \bfm{x}$'
- '$\def \X \bfm{X}$'
- '$\def \z \bfm{z}$'
- '$\def \thetab \boldsymbol{\theta}$'
- '$\def \betab \boldsymbol{\beta}$'
- '$\def \pib \boldsymbol{\pi}$'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



# Question 1:  Not So Simple Univariate Optimization

Let is revisit the problem from the last HW, now using BFGS to fit the model.  Report the results of the various starting values as last time, and comment on the convergence for each of the starting values relative the last HW that uses NR.  What properties about BFGS relative to NR could explain the different behavior in this setting? 

$$f(x) = 1.95 - e^{-2/x} - 2e^{-x^4}.$$

```{r}
# f(x)
f = function(x){
  value = 1.95 - exp(-2/x) - 2*exp(-x^4)
  return(value)
}

# first derivative
f1 = function(x){
  first = -2*x^(-2)*exp(-2/x) + 8*x^3*exp(-x^4)
  return(first)
}

x = 1.2 
library(optimx)
fit = optimx(
  par = x, # initial values for the parameters. 
  fn = function(x){f(x)}, # function to maximize
  gr = function(x){f1(x)}, # gradient/1st derivative
  method = "BFGS",
  control = list(
              trace = 0, # higher number print more detailed output
              maximize = T # default is to minimize
              )
)
print(fit)

x = 0.5
fit = optimx(
  par = x, # initial values for the parameters. 
  fn = function(x){f(x)}, # function to maximize
  gr = function(x){f1(x)}, # gradient/1st derivative
  method = "BFGS",
  control = list(
              trace = 0, # higher number print more detailed output
              maximize = T # default is to minimize
              )
)
print(fit)

x = 0.99
fit = optimx(
  par = x, # initial values for the parameters. 
  fn = function(x){f(x)}, # function to maximize
  gr = function(x){f1(x)}, # gradient/1st derivative
  method = "BFGS",
  control = list(
              trace = 0, # higher number print more detailed output
              maximize = T # default is to minimize
              )
)
print(fit)
```

Unlike NR that we have used in the previous homework, which has some divergence cases, BFGS algorithm converges in all three cases, and produces the same results regardless of the starting points. The property that could explain the different behavior in this setting is that, in BFGS, the backtracking approach can ensure ascent. On the other hand, the steps selected by NR may not necessarily go uphill.

## EM:  Zero-inflated Poisson 

Revisiting problem 3 from HW5, let us implement an EM-based maximization approach to estimate the model parameters.

Please define the CDLL, E-step, and M-step below as we did in class.   

Then, fill in the relevant portions of the code below. 

Hint for writing the CDLL:  Let $z_i = 1$ represent the true (known) membership to the non-fishing population, and $z_i = 0$ to represent membership to the fishing population.  Start with defining the complete data likelihood based on the non-aggregated likelihood below, then take the log to get the final CDLL form. This will help derive the forms for the E and M-steps.  For the actual fitting, we give some direction in the code below in terms of how to use the table aggregated data by a weighting approach. 

### Expression for Log Likelihood: from the previous HW

Lets rewrite the likelihood for the aggregated form of the data in terms of what it would look like when using the $n$ raw, non-aggregated responses:

$$ 
L(\boldsymbol{\theta}) = \prod_{i=1}^n (\pi + (1-\pi)e^{-\lambda})^{I[y_i=0]}\left((1-\pi)\frac{e^{-\lambda}\lambda^{y_i}}{y_i!}\right)^{I[y_i>0]}
$$

This is a simplified form of the PMF that was given at the beginning of the EM lecture. This corresponds to the following log-likelihood

$$\mathcal{l}(\boldsymbol{\theta}) = \sum_{i=1}^n I[y_i=0]\log(\pi + (1-\pi)e^{-\lambda}) + I[y_i>0]\left(\log(1-\pi) -\lambda + {y_i}\log(\lambda) + \log{y_i!}\right)$$

Therefore, if $y > 0$, we know automatically that that individual is from the fishing population.    


### Expression for Complete Data Log Likelihood: Solution

Start with the CDL.

Let $z_i$ denote the membership of subject $i$, such that $z_i = 1$ represents that subject $i$ belongs to the non-fishing population, whereas $z_i = 0$ represents that subject $i$ belongs to the fishing population. Let $\boldsymbol{\theta} =  (\pi, \lambda)$ be the parameters of interest. Then the complete data likelihood is represented as follows:

$$
L_c(\boldsymbol{\theta}) = \prod_{i=1}^n \pi^{I(z_i = 1)}((1-\pi)f(y_i \vert \lambda))^{I(z_i = 0)},
$$

where $f(y_i \vert \lambda)$ represents the Poisson density with mean $\lambda$. 

Now take the log.

We can express the complete data log likelihood as,

$$
\mathcal{l}_c(\boldsymbol{\theta}) = \log(\pi) \sum_{i=1}^n I(z_i = 1) + \log(1-\pi) \sum_{i=1}^nI(z_i = 0) +\sum_{i=1}^n I(z_i = 0) \log(f(y_i \vert \lambda)).
$$


### Expression for E-step: Solution

Now, we evaluate the conditional expectation of CDLL, we have that,

$$
Q(\boldsymbol{\theta} \vert \boldsymbol{\theta}^{(t)}) = \log(\pi) \sum_{i=1}^n p(z_i = 1 \vert \boldsymbol{y}_o, \boldsymbol{\theta}^{(t)}) + \log(1-\pi) \sum_{i=1}^np(z_i = 0 \vert \boldsymbol{y}_o, \boldsymbol{\theta}^{(t)}) +\sum_{i=1}^n p(z_i = 0 \vert \boldsymbol{y}_o, \boldsymbol{\theta}^{(t)}) \log(f(y_i \vert \lambda)).
$$
where

$$
p(z_i = 1 \vert \boldsymbol{y}_o, \boldsymbol{\theta}^{(t)}) = \frac{\pi^{(t)}}{\pi^{(t)} + (1-\pi^{(t)})\exp(-\lambda^{(t)})},
$$
$$
p(z_i = 0 \vert \boldsymbol{y}_o, \boldsymbol{\theta}^{(t)}) = \frac{(1-\pi^{(t)})\exp(-\lambda^{(t)})}{\pi^{(t)} + (1-\pi^{(t)})\exp(-\lambda^{(t)})}.
$$

### Expression for M-step: Solution

Here we maximize $Q(\boldsymbol{\theta} \vert \boldsymbol{\theta}^{(t)})$. We can simplify the problem as maximizing $\log(\pi) \sum_{i=1}^n p(z_i = 1 \vert \boldsymbol{y}_o, \boldsymbol{\theta}^{(t)}) + \log(1-\pi) \sum_{i=1}^np(z_i = 0 \vert \boldsymbol{y}_o, \boldsymbol{\theta}^{(t)})$ with respect to $\pi$, and maximizing $\sum_{i=1}^n p(z_i = 0 \vert \boldsymbol{y}_o, \boldsymbol{\theta}^{(t)}) \log(f(y_i \vert \lambda))$ with respect to $\lambda$. We then obtain the following update for $\pi$,

$$
\pi^{(t+1)} = \sum_{i=1}^n p(z_i = 1 \vert \boldsymbol{y}_o, \boldsymbol{\theta}^{(t)})/n,
$$

and the update of $\lambda$ can be obtained by fitting the weighted Poisson regression model with intercept only. Then $\lambda^{(t+1)}$ is obtained as the intercept estimate from the regression model with $\boldsymbol{\theta} =  \boldsymbol{\theta}^{(t)}$.

### Code implementation 

```{r}
# data 
y = 0:6
ny = c(3062, 587, 284, 103, 33, 4, 2)
## HINT:  to adjust using relative freq of counts in model/calculations when using aggregated data 
y_weight = ny/sum(ny) 
## For example
print(sum(ny*y)/sum(ny)) # mean of y based on aggregated data in table
## We get the same thing when fitting and intercept only poisson reg model, adjusting for relative freq of counts...
print(exp(glm(y ~ 1, weight = y_weight)$coef))
# to start the model
tol = 10^-8
maxit = 50
iter = 0
eps = Inf
ll = -10000
## create posterior probability matrix
pp = matrix(0,length(y), 2)
colnames(pp) = c("non-fisher", "fisher")
## initialize partion, everything  with count 0 is non-fisher, otherwise fisher
pp[which(y ==0),1] = 1
pp[,2] = 1 - pp[,1]
## now start the EM algorithm
while(eps > tol & iter < maxit){
  
  ## save old ll
    ll0 = ll
  
  ## start M-step
    # pi, 1 x 2 vector
    pi =colSums(pp*ny/sum(ny))

    # lambda, scalar
    lambda = glm(y ~ 1, weight = ny*pp[ ,2])$coef
  
  ## start E-step
    # update pp
    pp[1,1] = pi[1]/(pi[1]+pi[2]*dpois(0, lambda))
    pp[1,2] = pi[2]*ppois(0, lambda)/(pi[1]+pi[2]*dpois(0, lambda))
    
  ## calculate LL
    ll = ny[1]*log(pi[1]+(1-pi[1])*exp(-lambda)) +(sum(ny)-ny[1])*(log(1-pi[1]) - lambda) + sum(ny*y)*log(lambda)
      
  ## calculate relative change in log likelihood  
    eps  = abs(ll-ll0)/abs(ll0)
  
  ## update iterator
    iter = iter + 1
    if(iter == maxit) warning("Iteration limit reached without convergence")
  
  ## print out info to keep track
    cat(sprintf("Iter: %d logL: %.2f pi1: %.3f  eps:%f\n",iter, ll,pi[1],eps))
}
paste0("lambda = ", lambda , " ;pi = ", pi)[1]

```
