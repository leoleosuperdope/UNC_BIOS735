---
title: "Homework 12 - Support Vector Machines"
author: "Leo Li (PID: 730031954)"
date: "`r format(Sys.time(), '%m/%d/%Y')`"
output: html_document
---

# Credit card dataset

We will be working in this homework with a popular machine learning
dataset about credit card fraud. Go to the following link and download
the CSV:

<https://www.openml.org/d/1597>

The data description is 

> The datasets contains transactions made by credit cards in September
> 2013 by european cardholders. This dataset present transactions that
> occurred in two days, where we have 492 frauds out of 284,807
> transactions. The dataset is highly unbalanced, the positive class
> (frauds) account for 0.172% of all transactions. 
Now we begin by reading into R:

```{r setup, include=FALSE}
setwd("C:/Users/haoli/Desktop/735/Homework/HW12")
library(readr)
library(caret)
library(ggplot2)
library(RANN)
z <- read_csv("phpKo8OWT.csv")
```

```{r}
dim(z)
table(z$Class)
y <- gsub("\\'","",z$Class)
x <- as.data.frame(z[,-31])
```

We will deal with the class imbalance for this homework just by
downsampling the non-fraud cases. As we saw in the random forest
homework there are other approaches including custom cost functions. 

```{r}
set.seed(1)
idx <- c(sample(which(y == "0"), sum(y == "1")), which(y == "1"))
y <- y[idx]
x <- as.data.frame(scale(x[idx,]))
table(y)
```

The homework assignment is to run linear and radial basis function SVM
on the dataset, and report the Kappa for both models. For RBF, you
should plot the Kappa over the different values for the cost
function (`metric="Kappa"`). 

```{r}
# fit inear SVM
tg <- data.frame(C=seq(0.1,2,0.1))
linearSVM <- train(x, y, method="svmLinear", tuneGrid=tg)
linearSVM$results

# plot the kappa over the different values for the cost function
ggplot(linearSVM, metric="Kappa")

# fit RBF SVM
tg <- data.frame(C=seq(0.1,2,0.1), sigma=rep(0.05, 20))
RBFSVM <- train(x, y, method="svmRadial", tuneGrid=tg)
#RBFSVM$results[rownames(RBFSVM$bestTune),]
RBFSVM$results

# plot the kappa over the different values for the cost function
ggplot(RBFSVM, metric="Kappa")
```

Now, suppose we want to examine plots of the decision boundary in the
feature space. We can only look at two features at a time in a scatter
plot. What are the two most important variables for the SVMs (they are
the same for both SVMs)?

```{r}
# assess the variable importance for linear SVM
plot(varImp(object=linearSVM),   main="Linear SVM - Variable Importance")

# assess the variable importance for RBF SVM
plot(varImp(object=RBFSVM),   main="RBFSVM - Variable Importance")
```

From the plot, we can see that the most important variable in both cases are V12 and V14.

Make a scatterplot for each method that includes: the data points in this two
dimensional space, colored by the "0" and "1" prediction, and the decision
boundary. In class, we simply used `expand.grid` to build the
`newdata` that was fed to `predict`. Start with this approach, using a
grid of 40 points from -4 to 4 for the two most important variables,
but before you attempt to run `predict` (which would give an error), read further:

In this case, we have to worry about the other 30 - 2 = 28
variables. If we put in 0's, this would not be typical observations,
and we will get strange results.

Instead, you should put `NA` for the other variables, and use
`preProcess` with KNN imputation (alone, don't re-scale), to impute
the other values. Then use this data to run `predict` and define the
decision boundary. This is a simpler approach compared to the
integration approach taken by `plot.gbm` to produce marginal plots
that we saw when we looked at boosting, but it is sufficient to get a
sense of the decision boundary in 2D for "typical" values of the other
covariates. 

```{r}
length = 40
grid <- as.data.frame(matrix(NA, nrow = length^2, ncol = ncol(x)))
colnames(grid) = colnames(x)
s <- seq(from=-4,to=4,length=length)
grid2 <- expand.grid(V12=s,V14=s)
grid$V12 <- grid2$V12
grid$V14 <- grid2$V14
preProcValues <- preProcess(x, method = "knnImpute")
grid3 <- predict(preProcValues, grid)
grid4 <- grid3

# scatterplot for linear SVM
rsv <- as.data.frame(x[linearSVM$finalModel@SVindex,])
grid3$y <- predict(linearSVM, newdata=grid3)
grid3$yy <- 2*(as.numeric(grid3$y) - 1.5)
ggplot(x, aes(V12,V14,col=y)) + geom_point() + 
  geom_point(data=rsv, col="black", size=5, shape=21) +
  geom_contour(data=grid3, aes(V12,V14,z=yy), breaks=0, col="black") +
  geom_raster(data=grid3, aes(V12,V14,fill=y), alpha=.2)

# scatterplot for RBF SVM
rsv <- as.data.frame(x[RBFSVM$finalModel@SVindex,])
grid4$y <- predict(RBFSVM, newdata=grid4)
grid4$yy <- 2*(as.numeric(grid4$y) - 1.5)
ggplot(x, aes(V12,V14,col=y)) + geom_point() + 
  geom_point(data=rsv, col="black", size=5, shape=21) +
  geom_contour(data=grid4, aes(V12,V14,z=yy), breaks=0, col="black") +
  geom_raster(data=grid4, aes(V12,V14,fill=y), alpha=.2)
```

Do you see a big difference in the decision boundary for linear vs RBF
SVM? 

I do not see a big difference between the decision boundaries for linear and RBF SVM. However, there do exist some discrepancies. For example, at the right-hand side of the plot, where the decision boundary for linear SVM is tilted downwards, but that for RBF SVM is tilted somewhat upwards. 