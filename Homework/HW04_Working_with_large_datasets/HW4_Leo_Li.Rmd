---
title: "Homework 4 - Working with large datasets"
author: "Leo Li (PID: 730031954)"
date: "`r format(Sys.time(), '%m/%d/%Y')`"
output: html_document
---

# Question 1 - benchmark data.table's grouping on real data

Download the merged College Scorecard data from (2009-2016) from here: <https://www.dropbox.com/s/ex0u45rlrjr6h7e/Scorecard_2009-2016.csv?dl=0>

This file is the final merged form of the original data that was discussed in class, using the shell operation. Please use this file for the subsequent questions. Excluding the header, there should be 67,418 rows in this file. 

In class we performed some simple subsetting and grouping
operations. The lecture notes use a previous version of the dataset,
and since they were compiled, `CONTROL` is now integer valued, and
the `TUITFTE` and `SAT_AVG` columns will need to be coerced to a
numeric using `as.numeric`, before you can work with it. (This will
give a warning about NAs introduced, which you should ignore.)

Also you should convert `CONTROL` to a factor, and then change the
levels 1,2,3 to instead `pub`,`pnp`,`pfp`.

From the data dictionary, we have: 

| C | Value              |
|---|--------------------|
| 1 | Public             |
| 2 | Private nonprofit  |
| 3 | Private for-profit |

First, tabulate the number of schools you have in the table for each
value of `CONTROL` (you can use data.table or base R for this). Also
tabulate, the number of schools for each value of `CONTROL` that have
non-NA values for both `TUITFTE` *and* `SAT_AVG`.

```{r}
# import data
library(data.table)
library(plyr)
file <- "Scorecard_2009-2016.csv"
scores <- fread(file)
scores$TUITFTE <- as.numeric(scores$TUITFTE)
scores$SAT_AVG <- as.numeric(scores$SAT_AVG)
scores$CONTROL <- as.factor(scores$CONTROL)
scores$CONTROL <- revalue(scores$CONTROL, c("1" = "pub", "2" = "pnp", "3" = "pfp"))

# tabulate the number of schools for each value of CONTROL
scores[, .N, by = CONTROL]

# tabulate the number of schools for each value of CONTROL with non-NA values for TUITFTE and SAT_AVG
scores[(is.na(TUITFTE) == F)&(is.na(SAT_AVG) == F), .N, by = CONTROL]
```

Then, compute the mean and SD tuition per FTE and the mean and SD average
SAT for each of the classes of ownership (pub, pnp, pfp), (1) using
data.table, and (2) using `aggregate` with the columns `TUITFTE`,
`SAT_AVG`, `CONTROL` and your NA-removed mean and sd function. Confirm
by eye that they give the same result and compare speed. You can
benchmark with `times=10`.

A typical use of aggregate is:

```
aggregate(df[,c("col1","col2")], df[,"grouping"], function(x) ...)
```

```{r}
# define functions
mean2 <- function(x) mean(x, na.rm=TRUE)
sd2 <- function(x) sd(x, na.rm=TRUE)

# using data.table
using.data.table <- function(){
  mean_sd <- scores[, .(TUITFTE_mean = mean2(TUITFTE), TUITFTE_sd = sd2(TUITFTE),
                        SAT_AVG_mean = mean2(SAT_AVG), SAT_AVG_sd = sd2(SAT_AVG)), 
                    by = CONTROL]
  return(mean_sd)
}
using.data.table()

# using aggregate
using.aggregate <- function(){
  mean_sd <- merge(aggregate(scores[,c("TUITFTE","SAT_AVG")], 
                             scores[,"CONTROL"], mean2),
                   aggregate(scores[,c("TUITFTE","SAT_AVG")], 
                             scores[,"CONTROL"], sd2),by="CONTROL")
  colnames(mean_sd) <- c("CONTROL", "TUITFTE_mean", "SAT_AVG_mean",
                         "TUITFTE_sd", "SAT_AVG_sd")
  return(mean_sd)
}
using.aggregate()

# compare speed
library(microbenchmark)
microbenchmark(using.data.table(),using.aggregate(), times = 10)
```

# Question 2- doing more with "by" in data.table

Make a subset of the data, called `scores.sub`, which has complete
data for both `TUITFTE` and `SAT_AVG`. You can look up the `na.omit`
function in data.table.

```{r}
scores.sub <- na.omit(scores[,c("CONTROL", "TUITFTE", "SAT_AVG")])
```

Make a plot of `SAT_AVG` over `TUITFTE`, and color the points by
`CONTROL`, with x-limits of [0-40,000] and y-limits of [500-1600].

```{r}
library(ggplot2)
ggplot(scores.sub, aes(x = TUITFTE, y = SAT_AVG, color = CONTROL)) +
  geom_point() + xlim(0, 40000) + ylim(500, 1600)
```

Now tabulate the number of schools that have tuition per FTE over
20,000 and/or average SAT over 1200, grouped by ownership
category. Your output should be sorted on the groupings you define, so
the first row should be public, TUITFTE < 20,000 and SAT_AVG < 1200,
and so on for 12 rows. See the Introduction vignette for data.table
for insight on how to perform this operation. Hint: "sorted by" and
"expressions in by".

```{r}
scores.sub[, .N, keyby = .(CONTROL, TUITFTE > 20000, SAT_AVG > 1200)]
```

# Question 3 - subsets of data 

Use data.table to obtain the tuition per FTE and average SAT for the
two schools with the top average SAT within each ownership
group. Hint: I performed this in two steps, first by ordering
`scores.sub`, and then using "subset of data". Make sure to avoid
returning all of the columns...

```{r}
sorted.score.sub <- scores.sub[order(CONTROL, -SAT_AVG)]
sorted.score.sub[, head(.SD, 2), by = CONTROL]
```

# Question 4 - MovieLens sparse dataset

As we mentioned in class, one common form of sparse data is when we
have information about individuals and their interaction with a large
set of items (e.g. movies, products, etc.). The interactions may be
ratings or purchases. One publicly available dataset of movie ratings
is *MovieLens*, which has a 1 MB download available here:

<https://grouplens.org/datasets/movielens/>

Download the `ml-latest-small.zip` dataset. Take a look at each of the
CSV files. How many of the movies have the "Comedy" genre attached to
them? 

```{r}
# import data
file <- "movies.csv"
movies <- fread(file)

# count the number of movies that have the "Comedy" genre attached
movies[, .N, grepl("Comedy", genres)]
# From the output, there are 3756 movies which have the "Comedy" genre attached.
```


Build a sparse matrix of the movies by users, and just put a 1 for if
the user rated the movie (don't actually record the value of the
rating itself). You can do this by specifying `x=1`. In
the abstract, this is a very large matrix, but this is because the
user IDs go up to nearly 200,000. Remove the rows of the sparse matrix
where there are no ratings to produce a sparse matrix that is roughly
~10,000 by ~600. Use `summary` to investigate the range, quartiles,
etc. of number of movies rated by each user.

```{r}
# import data
file <- "ratings.csv"
ratings <- fread(file)

# create a sparse matrix
library(Matrix)
movies.by.users <- sparseMatrix(i = ratings$movieId, j = ratings$userId, x=1)
movies.by.users <- movies.by.users[(rowSums(movies.by.users)>0), ]

# investigate the range, quartiles, etc. of number of movies rated by users
summary(colSums(movies.by.users))
```

There are multiple ways to compute the SVD of a sparse matrix. If
after manipulating the matrix in its sparse form, it is not too large
(as in this case), one can just run `svd` which will coerce the matrix
into a dense one. Or there are special functions in packages which are
designed to compute (potentially sparse) SVD solutions on sparse
matrices. Two such functions are `sparsesvd::sparsesvd` and
`irlba::ssvd`. You can choose any of these three methods, in either
case you should specify to return only 3 left singular vectors
(`nu=3`, `rank=3`, or `k=3`, respectively). For `ssvd` in the irlba
package, you should specify that the number of nonzero components
in the right singular vectors should be all (the number of rows of x),
which will give a warning that you should ignore. All of these methods
will produce roughly the same decomposition, with arbitrary sign
changes on the singular vectors. The sparse versions are about 1000
times faster, as they do not coerce the matrix into a dense version.

Compute the SVD of the matrix using one of the methods above. Plot the
columns of the U matrix against each other: 1 vs 2, 2 vs 3, 1
vs 3. Note that column 1 and 3 are correlated, with a long tail of
movies. Investigate the names of these movies. What property can you
infer about the top 6 movies in the tail w.r.t. column 1 and 3? Now
look at the extremes of column 2 of U. What difference can you tell
about the movies, between the smallest values and the largest values
in column 2?

Hint: there are a few movies which are in the `movies.csv` file, but
are not in the `ratings.csv` file. I recommend to subset the list of
movies first, which will help with this problem.

```{r}
# subset the list of movies
movies.subset <- movies[(movies$movieId %in% ratings$movieId), ]

# compute the SVD of the matrix
library(sparsesvd)
mat.svd <- sparsesvd(movies.by.users, rank = 3)

# plot the columns of the U matrix against each other
plot(mat.svd$u[,1], mat.svd$u[,2])
plot(mat.svd$u[,2], mat.svd$u[,3])
plot(mat.svd$u[,1], mat.svd$u[,3])

# names of the movies on the tail w.r.t. column 1 and 3
tail.1.3 <- scale(mat.svd$u[,1]) + scale(mat.svd$u[,3])
whichpart <- function(x, n=30) {
  nx <- length(x)
  p <- nx-n
  xp <- sort(x, partial=p)[p]
  which(x > xp)
}
movies.subset[whichpart(tail.1.3,6), ]
```

In this question, the movies in the tail w.r.t. column 1 and 3 are identified
by taking the summation of the corresponding standardized values of column 1 
and 3. The six movies with the greatest values of the summation are identified 
as above. One property shared by the movies identified is that they are all 
very famous movies rated by a lot of people. 

```{r}
# smallest values in column 2
movies.subset[whichpart((-mat.svd$u[,2]),8), ]

# largest values in column 2
movies.subset[whichpart(mat.svd$u[,2],8), ]
```

From the output above, the movies with the smallest values in column 2 have
mostly been released during the 1990s, whereas the movies with the largest
values in column 2 have mostly been released during the 2000s. 
